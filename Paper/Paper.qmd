---
title: "Tails or Types? A Critical Systematic Review of Taxometric Studies in Neurodevelopmental Disorders"
shorttitle: "Taxometric Review Neurodevelopmental Disorders"
authors:
  - name: Enrico Toffalini
    corresponding: true
    orcid: 0000-0002-1404-5133
    email: enrico.toffalini@unipd.it
    affiliation: Department of General Psychology, University of Padua, Italy
  - name: Margherita Calderan
    corresponding: false
    orcid: 0009-0005-5668-5162
    email: margherita.calderan@unipd.it
    affiliation: Department of General Psychology, University of Padua, Italy
  - name: Riccardo Pagan
    corresponding: false
    email: riccardo.pagan.3@studenti.unipd.it
    affiliation: Department of Social and Developmental Psychology, University of Padua, Italy
  - name: Valentina Tobia
    corresponding: false
    email: tobia.valentina@unisr.it
    affiliation: University Vita-Salute San Raffaele, Milan, Italy
author-note:
  disclosures:
    study-registration: 'The review was preregistered at https://osf.io/65y9g/?view_only=91e97c6b00dc444abb83fefcf67cbac0'
    financial-support: 'Financial support: Project 2022KBW99S "Tails or types? Testing the dimensional hypothesis in neurodevelopmental disorders", funded by Next Generation EU, Mission 4, Componente 1, CUP C53D23004210006'
    gratitude: "We are grateful to Athanasia Vasileiadou who first pointed us to some of the reviewed articles."
    data-sharing: https://osf.io/ys5ad/?view_only=0e2812acfabf43fc8af01f883c7331ba
    related-report: null
    conflict-of-interest: null
    authorship-agreements: null
abstract: "Taxometric methods are designed to test whether latent structures underlying psychological constructs are categorical (taxonic) or dimensional. Despite their relevance to the nature of neurodevelopmental disorders (NDDs), their application has been limited and often methodologically flawed. We conducted a preregistered systematic review to critically evaluate this literature. The review identified all published taxometric studies on autism, ADHD, learning, and language disorders, with a focus on methodological biases such as artificial admixture. While studies on ADHD and SLI supported dimensional structures, most studies on autism and NDD subtypes reported taxonic findings, often in the presence of methodological threats. A Monte Carlo simulation illustrates how artificial admixture can spuriously generate taxonic results in otherwise dimensional data. We conclude that evidence for categorical models in NDDs is frequently undermined by methodological artifacts, reinforcing the need for caution, but also advocating for more, high-quality taxometric studies in the field of NDDs."
keywords: [Neurodevelopmental disorders, Taxometric analysis, Autism, ADHD, Learning disorders]
bibliography: references.bib
format:
  apaquarto-docx: default
  apaquarto-html: default
  apaquarto-typst: default
  apaquarto-pdf:
    documentmode: man
---

```{r,message=F,warning=F}
library(readxl)
library(kableExtra)
library(stringr)
library(dplyr)
```

In recent years, research on neurodevelopmental disorders (NDDs) has undergone significant reconsideration. Traditional diagnostic categories defined in the DSM-5 [@dsm5] have increasingly been questioned in favor of a dimensional and transdiagnostic perspective [e.g., @astle2022trandiagnostic; @michelini2024world; @happe2020looking; @happe2021both; @sonuga2020kuhnian; @caviola2024editorial]. Notably, the DSM-5 itself has incorporated some dimensional features (for example by defining a broad Autism Spectrum Disorder [ASD] in place of distinct diagnoses). Several factors have motivated this shift: the marked heterogeneity within diagnostic groups, the weak correspondence between data-driven clusters and DSM-based categories, the frequent comorbidity and phenotypic correlations across conditions, and the view that individuals with NDDs may represent, at least in part, the tails of developmental continua, quantitatively rather than qualitatively different from the so-called neurotypical population [@michelini2024world; @happe2021both; @mammarella2021nocore].

Determining whether NDDs truly represent categories, rather than just tails of continuous distributions spanning the general population, is important for several reasons, including questions of social and personal identity for affected individuals, implications for diagnostic framing, comorbidity, and intervention choices in clinical contexts, and consequences for the organization of classrooms, educational practices, and even the design and power analysis of research studies [@caviola2024editorial]. But more fundamentally, this question is relevant for our understanding of the ontological status of psychological phenomena.

A dimensional approach aligns well with long-standing principles in the study of individual differences in psychology, and has long characterized research on intelligence and personality [@caviola2024editorial]. It also seems well-suited for NDDs, given a growing body of evidence supporting polygenicity and multifactorial etiology [@plomin2005generalist; @kovas2006generalist; @pennington2012multiple; @astle2020core; @caviola2024editorial; @demontis2023adhdgenome]. Most behavioral traits are understood to result not from one or a few causes, but from the additive effects of many small influences [@chabris2015fourth]. The central limit theorem [@fischer2011central] posits that, when many individual causes (such as discrete genetic variants) contribute additively, the resulting distribution tends to continuity and normality (this is, in fact, the "secret" of the Gaussian distribution). Nonetheless, a principled categorical view still resists in neurodevelopmental research, for example concerning ASD [e.g., @happe2021both; @chown2021bit; @frazier2023asd], subtypes within NDDs [e.g., @burgess2018taxodyslexia] or the proposed emergence of data-driven taxa [@astle2022trandiagnostic].

Taxometric analysis is a set of quantitative methods originally introduced by Paul Meehl and colleagues in the 1970s and refined over decades [e.g., @meehl1995taxometric], explicitly aimed at establishing whether a clinical disorder is characterized by an underlying categorical or dimensional structure. It has been widely applied in psychopathology, and typically suggested that most mental health conditions (e.g., depression, anxiety) are better conceptualized as dimensional [@haslam2012review; @haslam2020metaanalysis]. In brief, taxometric procedures, such as MAMBAC, MAXCOV, MAXEIG, and L-Mode examine covariation patterns among observed indicators to compare competing latent models: one categorical, the other dimensional [@ruscio2007taxometric; @ruscio2004checklist; @cole2004taxometric]. These methods now feature simulation-based tools like the Comparison Curve Fit Index (CCFI), which provide a quantitative measures to facilitate inference [@ruscio2018using].

Unfortunately, the use of taxometric analysis has remained limited in neurodevelopmental research, which most existing studies focusing on adult psychopathology [@beauchaine2003taxometrics; @haslam2020metaanalysis]. In their debate on whether ASD is best conceptualized as a category or a continuum, @happe2021both and @chown2021bit do not even mention such methods. Similarly, in their influential review, @astle2022trandiagnostic briefly acknowledge taxometry, but puts much more emphasis on studies using cluster analysis. However, the limitations of clustering methods for assessing taxonicity have been discussed for decades: clustering algorithms tend to impose (rather than test for) categorical structures, they provide weak tests of taxonicity, and often fail to recover known taxa in simulations [@beauchaine2003taxometrics; @ruscio2004boundaryissues]. Recent simulations using psychologically realistic data (featuring both skewness and intercorrelated indicators) suggest that commonly used clustering methods are prone to detecting pseudoclusters while presenting low power to identify true taxa [@toffalini2022entia; @toffalini2024clusters]. Indeed, @astle2022trandiagnostic interpret clustering results as evidence that data-driven groups do not align with DSM-5 categories, rather than as compelling tests of latent structural form.

Taxometric analysis is not without limitations, some of which are shared with cluster analysis. Commonly cited threats to validity of conclusions include poor indicator validity, nuisance covariance, low base rate, and artificial admixture [@cole2004taxometric; @haslam2020metaanalysis; @ruscio2004checklist; @ruscio2018using].

Poor indicator validity implies that indicators present little mean difference between individuals with and without the target condition (i.e., the putative taxon and complement). Ideally, Cohen’s *d* for each indicator should substantially exceed 1 (*d* $\gg$ 1). Nuisance covariance refers to strong residual correlations among indicators after accounting for group membership (indicating redundancy among variables). Low base rate means that too few individuals in the sample belong to the taxon, making accurate detection difficult (samples with *n* < 30 for the taxonic group are usually deemed insufficient). This is particularly relevant when studying rare diagnostic categories. Artificial admixture occurs when participants are combined from different recruitment sources or selection criteria, often as an attempt to compensate for a low base rate. For instance, clinical cases may be recruited separately from controls and merged into a single sample. In general, poor indicator validity and low base rate reduce power to detect true taxa, while high nuisance covariance and artificial admixture increase the risk of detecting pseudotaxa by mimicking a categorical structure.

## Goals of the Present Review

The primary aim of this systematic review is to critically examine the use of taxometric methods in studies of NDDs within the published literature. We focused on the most commonly investigated NDD categories: Autism Spectrum Disorder (ASD), Attention-Deficit/Hyperactivity Disorder (ADHD), and all forms of Specific Learning Disorders (SLDs), along with the closely related condition of Language Disorder/Impairment. (Language disorders were not included in the preregistered protocol and therefore constitute a minor deviation.) We excluded Intellectual Disability, as its etiology is often clearly identifiable and taxonic/syndromic by definition in many cases (e.g., Down syndrome), or linked to specific medical risk factors, while residual non-syndromic cases are typically regarded as reflecting the lower tail of general mental ability and thus inherently dimensional.

A crucial goal was to assess the methodological quality and credibility of taxometric findings. We specifically evaluated potential sources of bias, such as artificial admixture, and assessed whether studies provided sufficient methodological information to evaluate other known threats to validity, including low indicator validity, high nuisance covariance, and skewed indicator distributions. (The latter is particularly relevant when scales assess rare or extreme behaviors, which can create the illusion of a categorical structure.)

Following the review, we report a Monte Carlo simulation study designed to empirically demonstrate the extent to which artificial admixture can lead to the spurious detection of categorical latent structures in otherwise dimensional data, using both taxometric and clustering methods.

# Systematic Review of Taxometric Analysis on NDDs

## Methods

The systematic review was conducted in accordance with the Preferred Reporting items for Systematic Reviews and Meta-Analysis (PRISMA) 2020 guidelines. A review protocol was preregistered on OSF registries at the following link: https://osf.io/65y9g/?view_only=91e97c6b00dc444abb83fefcf67cbac0 The methods are detailed below.

### Sources and Search strategy

A systematic search for relevant peer-reviewed articles was conducted in April 2025 using four electronic databases: PsycINFO, PubMed, Scopus, and Web of Science. No time limits were applied, although records predating 1990 were not expected, as taxometric methods were not yet widely adopted at that time. The following search query—reflecting the methods and target populations of interest—was applied to titles, abstracts, and keywords:

```
(("taxometric*" OR "MAMBAC" OR "MAXCOV" OR "MAXEIG" OR "L-Mode") AND 
("autis*" OR "ASD" OR "attention-deficit*" OR "ADHD" OR "learning dis*" 
OR "reading dis*" OR "dyslexia" OR "math* dis*" OR "math* diff*"))
```

In databases where the wildcard `*` was not supported (e.g., PubMed), it was replaced with full terms (e.g., `"disorder"`, `"disability"` instead of `"dis*"`; `"mathematical"`, `"mathematics"`, `"math"` instead of `"math*"`).

Language disorder was subsequently added as a deviation from the preregistered protocol, replacing the second part of the search query with:

```
("language imp*" OR "language dis*")
```

Additionally, the full search query was launched on Google Scholar, and the first 10 pages of results were screened manually. References cited in, and articles citing, @haslam2020metaanalysis were also screened via Google Scholar. Finally, an exploratory search using OpenAI’s GPT-4o was conducted, using the preregistration protocol as contextual input. All promising records identified through these additional methods and not retrieved via database searches were listed under the *"Identification of studies via other methods"* section of the PRISMA flowchart.

### Eligibility Criteria

Inclusion criteria were as follows:  
1. The study applied at least one taxometric procedure (e.g., MAMBAC, MAXCOV, L-Mode, MAXEIG);  
2. The target population consisted of individuals with NDDs or related traits;  
3. The study drew a conclusion about the latent structure (categorical vs. dimensional), or reported sufficient information (e.g., CCFIs) for an independent reader to infer such a conclusion.

Exclusion criteria were:  
1. The article was theoretical, methodological, or a simulation study without empirical data;  
2. The study did not report any behavioral data (e.g., only neuroimaging or neurophysiological data; note: this criterion was later reconsidered, but no studies were excluded for this reason in practice);  
3. The study was a review and/or meta-analysis;  
4. The study employed latent structure methods other than taxometric analysis (e.g., latent profile analysis, mixture modeling), with no taxometric procedures reported.

In addition, consistent with the preregistered protocol, the search was limited to published, peer-reviewed journal articles. This decision reflected the primary aim of providing a critical review of the existing empirical literature. However, one potentially relevant non-journal record was identified and, while excluded from the systematic review, is discussed separately in the *Discussion* section.

### Screening and Coding

Two independent human coders (MC and RP) screened and coded all articles. No AI tools were involved in the coding process, except in occasional cases where coders used them to help clarify specific article content. All discrepancies were resolved through discussion and later reviewed by the principal investigator (ET).

Since a single article could report multiple (sub)samples and/or multiple taxometric analyses on the same sample, the full coded dataset followed a long-form data structure, with some articles appearing across multiple rows. Coded variables were defined in the data dictionary included in the preregistration protocol, with a few additional fields added to capture coder notes and agreement metrics.

Inter-coder agreement was fair to good across all phases. In the title and abstract screening phase, Cohen’s *κ* = 0.79 (34 out of 38 independent decisions matched). In the full-text eligibility screening, Cohen’s *κ* = 0.52 (15 out of 19). For the final data extraction, 84% of rows (71 out of 85) were extracted by both coders. Agreement on critical variables was high: for the "authors' taxonic conclusion" agreement was 94%; for "artificial admixture", it was 95% at the sample or subsample level.

## Results

A total of 110 records were identified through database searches. After removing 67 duplicates and excluding 5 additional records (dissertations or book chapters, only one of which appeared to include primary taxometric analyses but could not be retrieved), 38 unique articles were screened at the title and abstract level. 

Nineteen records were excluded at this stage. Full-text assessment was then conducted on the remaining 19 articles, of which 7 were excluded for not meeting the inclusion criteria: 4 did not apply taxometric methods, and 3 did not target neurodevelopmental disorders.

An additional 4 records were identified through citation tracking and online browsing. Of these, 2 were excluded (1 was a methodological article, and 1 did not employ taxometric analysis).

In total, 14 studies met inclusion criteria and were included in the review. The PRISMA flowchart in @fig-prisma summarizes the study selection process.

@fig-prisma *about here*

![PRISMA 2020 flow diagram illustrating the study screening and selection process](Figures/PRISMA_flow_diagram.png){#fig-prisma}

```{r}
df = data.frame(read_excel("../Literature-review/data/data_extraction.xlsx"))
```

A summary of the included studies, simplified to one row per article, is presented in @tbl-summary. A full reporting of all coded data for each individual taxometric analysis is available in the Supplemental Online Material. The total number of taxometric analyses was `r nrow(df)`, ranging from `r min(table(df$ID_article))` to `r max(table(df$ID_article))` analyses per article. The targeted disorders were ADHD (6 articles), ASD (6 articles), dyslexia (1 article), and Specific Language Impairment (SLI; 2 articles), with one article addressing both ADHD and ASD. Most articles (10 out of 14) examined whether a given disorder constituted a taxon, while the remaining 4 focused on whether putative subtypes within a disorder exhibited taxonic structure.

The most commonly used taxometric method was MAMBAC, applied in 13 out of 14 articles and in `r sum(grepl("MAMBAC", df$Taxometric_methods_used))` out of `r nrow(df)` total analyses. This was followed by MAXEIG (9 articles), L-Mode (7 articles), MAXCOV (2 articles), and MAXSLOPE (1 article). Most studies employed more than one taxometric procedure, with a median of `r median(str_count(df$Taxometric_methods_used, ";") + 1)` methods per analysis. In 11 out of 14 articles, interpretation was based on a quantitative fit index (Fit~d~ or, in more recent studies, the CCFI), while the remaining 3 relied on visual inspection of taxometric curves. The number of indicators entered into analyses ranged from `r min(df$Indicators_number, na.rm = TRUE)` to `r max(df$Indicators_number, na.rm = TRUE)`, with a median of `r median(df$Indicators_number, na.rm = TRUE)` (excluding one article in which the number was not clearly reported). The median sample size across all analyses was `r median(df$Sample_size)`, or `r median((df |> group_by(ID_article) |> summarize(avg = mean(Sample_size)))$avg)` when averaged within articles.

Taxonic conclusions were mixed. All five articles targeting ADHD as a disorder concluded in favor of a dimensional structure, as did both articles focusing on SLI. In contrast, all four articles examining ASD as a disorder leaned toward, or explicitly supported, a categorical (taxonic) structure. The remaining four articles investigated subtypes of disorders, and all leaned toward taxonic conclusions—either strongly or predominantly [@stevens2018functional on ADHD subtypes; @munson2008asd on ASD subtypes; @obrien2012dyslexia on dyslexia subtypes], or partially [@ingram2008defining, which supported ASD subtypes as taxa depending on the set of indicators used]. In total, 8 out of 14 articles reached a taxonic conclusion in at least some of their taxometric analyses. Specifically, `r sum(df$Taxonic_conclusion)` out of `r nrow(df)` total analyses—representing `r round(mean(df$Taxonic_conclusion) * 100, 1)`%—yielded taxonic results. These were primarily associated with ASD: `r sum(grepl("ASD", df$Target_disorder) & df$Taxonic_conclusion == 1)` out of `r sum(grepl("ASD", df$Target_disorder))` analyses involving ASD (i.e., `r round(100 * sum(grepl("ASD", df$Target_disorder) & df$Taxonic_conclusion == 1) / sum(grepl("ASD", df$Target_disorder)), 1)`%) supported a categorical structure.

Artificial admixture was evident or strongly suspected in at least part of the analyses in 8 out of 14 articles, involving a total of `r sum(df$Artificial_admixture)` out of `r nrow(df)` analyses. Notably, articles that incurred artificial admixture tended to align with those that reached taxonic conclusions: 6 of the 8 articles with suspected admixture also reported taxonic findings. This association was even more pronounced at the level of individual analyses. A taxonic conclusion was reached in `r sum(df$Taxonic_conclusion[df$Artificial_admixture == 1])` out of `r sum(df$Artificial_admixture == 1)` analyses (`r round(100 * mean(df$Taxonic_conclusion[df$Artificial_admixture == 1]), 1)`%) where admixture was evident or suspected. In contrast, only `r sum(df$Taxonic_conclusion[df$Artificial_admixture == 0])` out of `r sum(df$Artificial_admixture == 0)` analyses (`r round(100 * mean(df$Taxonic_conclusion[df$Artificial_admixture == 0]), 1)`%) reached a taxonic conclusion when admixture was likely or certainly absent.

Remaining methodological issues concerned missing information about key descriptive statistics and validity indices. In particular, **nuisance covariance**—that is, correlations among indicators within diagnostic groups or within the putative taxon and complement—could be coded for only 2 out of 14 articles. Where reported, it was small to negligible in @james2016latent, but exceeded 0.40 for many indicators in @frazier2023asd; notably, both studies reached a taxonic conclusion. **Skewness** of indicator score distributions was reported in only 5 articles. When available, skewness values were often near or beyond ±1 for at least one indicator in roughly one-third of analyses. However, there was no clear association between skewness and whether a taxonic conclusion was reached. By contrast, **indicator validity** was more consistently reported, and could be coded in all but 3 articles. Where available, validity (typically expressed as Cohen’s *d*) was almost always large or very large (*d* $\gg$ 1), consistent with best-practice recommendations for taxometric analysis.

@tbl-summary *about here*

```{r}
#| label: tbl-summary
#| tbl-cap: Overview summary of the reviewed articles."
summary_tab = read_excel("Tables/Review-Summary.xlsx",sheet=2)
kable(summary_tab, format = "latex", booktabs = TRUE) |>
  kable_styling(font_size = 8, latex_options = c("striped", "scale_down")) |>
  column_spec(1:ncol(summary_tab),width="5em") |>
  landscape()
```

## Additional Use of Cluster Analysis

Among the 14 identified articles, 6 also employed clustering methods—typically latent class analysis (LCA) or latent profile analysis (LPA). In 5 of these 6 cases, clustering was conducted in parallel with taxometric analysis [@james2016latent; @munson2008asd; @frazier2023asd; @frazier2007adhd; @deserno2023probing], while in one case it was presented as a secondary analysis [@frazier2010asd]. All six studies targeted ADHD and/or ASD, and in all cases clustering results were interpreted as supporting taxonic conclusions. These interpretations generally converged with those from taxometric analysis, with one exception: in @frazier2007adhd, clustering results were partially inconclusive but suggested the presence of subgroups within ADHD, thus contrasting with the dimensional structure favored by the taxometric analysis. In all but two studies [@frazier2007adhd; @munson2008asd], we identified at least some risk of artificial admixture.


# Simulation of Artificial Admixture

We focus on artificial admixture because it represents a powerful threat to validity that is both common (as suggested by our review) and avoidable by design through better planning of a study. Different types of admixture can be incurred. @ruscio2004checklist describes three main forms. First (and most frequently) is the case where clinical and nonclinical participants are separately recruited and merged into a single sample. Second, admixture can occur when a sample is artificially split into subsamples based on different selection criteria, and these are then used to test different taxometric questions [cf. "subtractive" compound sampling in @haslam2020metaanalysis]. Third, researchers may trim observations from the putative complement (i.e., individuals without the disorder) to inflate the base rate of the taxon. Each of these procedures risks imposing a categorical structure onto data that might be dimensional in nature. It is worth noting that artificial admixture poses a threat not only to taxometric analysis, but also to clustering methods, for the same reasons: both infer latent structure based on observed covariation, which can be distorted by selection artifacts.


```{r}
load("../R/simulateAdmixtResults.RData")
```

We simulate a dimensional scenario to visually demonstrate how artificial admixture, specifically of the first type described above, can produce pseudotaxonic results. A single normally distributed latent variable $X$ is generated to represent the underlying clinical trait of interest. A set of observed indicators ($x_1$, $x_2$, $x3$) are then simulated as linear functions of $X$ plus normally distributed random noise, yielding an $R^2$ of 0.77 for each indicator. Individuals above the 99th percentile of $X$ are assumed to receive a diagnosis via global clinical assessment. A sample of $N = 1000$ individuals is drawn from the general population. Given the low base rate, only about $n = 10$ cases are expected to naturally receive a diagnosis. To address this, researchers draw a second sample consisting of an additional $n = 1000$, targeting clinically diagnosed cases, and merge it with the original sample, thereby creating an artificial admixture. The full simulation code and data are available in the online materials:  
<https://osf.io/ys5ad/?view_only=0e2812acfabf43fc8af01f883c7331ba>

An instance of the simulation, illustrating two observed variables $x_1$ and $x_2$, is presented in @fig-admixture. Color is used to distinguish individuals with and without a diagnosis, but even without this aid, two clusters visually emerge. This apparent separation is misleading: no latent categories exist in the data. The appearance of a categorical structure is a consequence of artificial admixture. In this simulated example, indicator validity is high (Cohen’s *d* ≈ 2), suggesting strong separation between groups on each variable. However, nuisance covariance is partly problematic: among diagnosed individuals, the correlation between indicators is close to zero ($r \approx 0.05$), whereas among non-diagnosed individuals, it is substantial ($r \approx 0.60$). This asymmetry, combined with differential recruitment, contributes to the illusion of a latent taxon.

@fig-admixture *about here*

![A simulated example of artificial admixture, where a large number of individuals with a diagnosis are separately recruited and added to the sample (total N = 2000). The true underlying structure is strictly dimensional.](Figures/figureAdmixture.png){#fig-admixture}

A Monte Carlo simulation was conducted with 1000 iterations to assess the impact of artificial admixture on taxometric outcomes. Analyses were performed using the `RTaxometric` package in R. As recommended in the literature, inference was based on the Comparison Curve Fit Index (CCFI), which ranges from 0 (indicating strong support for a dimensional structure) to 1 (indicating strong support for a categorical structure), with values around 0.5 considered ambiguous. The default procedures (MAMBAC, MAXEIG, and L-Mode) were applied to each simulated dataset. To enable all three procedures, three observed indicators ($x_1$, $x_2$, $x_3$) were included in the simulations.

Over 1000 simulated datasets, results consistently and strongly favored a categorical interpretation: median combined CCFI = `r formatC(median(resultsAdmix$CCFI.mean),2,format="f")`, 95% quantile interval \[`r formatC(quantile(resultsAdmix$CCFI.mean,.025),2,format="f")`, `r formatC(quantile(resultsAdmix$CCFI.mean,.975),2,format="f")`\]; median MAMBAC CCFI = `r formatC(median(resultsAdmix$CCFI.MAMBAC),2,format="f")`; average MAXEIG CCFI= `r formatC(median(resultsAdmix$CCFI.MAXEIG),2,format="f")`; average L-Mode CCFI = `r formatC(median(resultsAdmix$CCFI.LMode),2,format="f")`. The smallest combined CCFI over 1000 iterations was `r formatC(min(resultsAdmix$CCFI.mean),2,format="f")`. Therefore, admixture can be an extremely strong driver of false categorical findings, especially when many clinical cases are separately recruited and added to a sample.

To verify that artificial admixture also produces misleading results when using clustering methods, we ran Gaussian Mixture Models (GMMs) comparing 1-component versus 2-component solutions, using the `mclust` package in R. Model selection was based on the Bayesian Information Criterion (BIC), as per default in GMMs. GMM is a form of model-based clustering grounded in finite mixture modeling. It is conceptually similar to latent profile analysis (LPA; with which it is often confused) but is more flexible in that it models covariances among indicators. Across 1,000 simulated datasets, results consistently favored the 2-component solution over the unimodal alternative. The median difference in BIC (ΔBIC) was `r formatC(median(resultsAdmix$gmmBIC1 - resultsAdmix$gmmBIC2), 2, format = "f")`, in favor of the 2-component model.

To confirm that the categorical structure was incorrectly favored specifically due to artificial admixture, we run a second Monte Carlo simulation using the same dimensional data-generating process as before, but without admixture. In this case, a single sample of $N = 2000$ observations was drawn at each iteration. Therefore, nuisance covariance remains present, but there is no admixture. Median combined CCFI = `r formatC(median(resultsNoAdmix$CCFI.mean),2,format="f")`, 95% quantile interval \[`r formatC(quantile(resultsNoAdmix$CCFI.mean,.025),2,format="f")`, `r formatC(quantile(resultsNoAdmix$CCFI.mean,.975),2,format="f")`\]; median MAMBAC CCFI = `r formatC(median(resultsNoAdmix$CCFI.MAMBAC),2,format="f")`; average MAXEIG CCFI= `r formatC(median(resultsNoAdmix$CCFI.MAXEIG),2,format="f")`; average L-Mode CCFI = `r formatC(median(resultsNoAdmix$CCFI.LMode),2,format="f")`. Clustering with GMM also consistently favored the 1-component model (over the 2-component model), with an average ΔBIC = `r formatC(median(resultsNoAdmix$gmmBIC2-resultsNoAdmix$gmmBIC1),2,format="f")` in favor of the 1-component model.

Thus, without admixture taxometric results leaned towards a dimensional conclusion, but frequently remained inconclusive. This is probably due to nuisance covariance. It is worth noting, however, that under a dimensional structure achieving both high indicator validity and low within-group covariance might be practically impossible. This is because indicators that clearly differentiate individuals with and without a diagnosis must be strongly associated with the underlying trait, an association that produces inter-indicator covariance. In fact, this trade-off may iteself serve as indirect evidence for dimensionality.

# Discussion

We found that taxometric analysis remains underused in neurodevelopmental research, which is unfortunate given the unique relevance of this method to the ongoing debate about the nature of NDDs [@astle2022trandiagnostic; @michelini2024world; @happe2021both; @sonuga2020kuhnian]. That underuse was expected, however. @haslam2020metaanalysis classified only 15 out of 183 taxometric studies in the "childhood" category (which included heterogeneous conditions, and even sleep problems). In the studies identified in our review, only ASD, ADHD, and SLI were targeted as disorders in taxometric analysis. None of learning disorders was targeted as a taxon, with Dyslexia being covered in one single article, but only for investigating its subtypes.

Taxometric results appeared relatively consistent. Dimensional conclusions were predominantly reached for ADHD and SLI, while categorical conclusions prevailed for ASD and studies examining subtypes of any disorder. This echoes @haslam2020metaanalysis, and may seem to offer a compelling answer to the debate raised by @chown2021bit and @happe2021both. However, a closer inspection of the methodological limitations in the reviewed studies suggests that such conclusions should be interpreted with caution.

We identified four studies that targeted ASD as a disorder in taxometric analyses, and all of them reached taxonic conclusions. However, some artificial admixture was identified in all of them, and it represents a major threat to conclusion validity, as demonstrated both in our data simulation and in prior literature [@ruscio2004checklist; @ruscio2004boundaryissues]. Admixture in the reviewed studies ranged from sampling participants through different methods, only some of which explicitly targeted clinical populations [@james2016latent], to combining clinical and non-clinical samples [@frazier2023asd; @deserno2023probing]. In one case, inclusion criteria from a larger database were partly unclear, but the sample was explicitly structured by diagnostic status [@deserno2023probing]. Another form of admixture involved constructing a sample of affected and unaffected siblings [@frazier2010asd]. According to our interpretation, the only relatively unbiased taxometric evidence suggesting that ASD may be a taxon as a disorder comes from a secondary analysis by @frazier2010asd, in which only unaffected siblings were examined.

Concerning subtypes, the situation was similar. All four reviewed articles reached some taxonic conclusions, but three of them likely involved some form of artificial admixture. However, the issue appeared to be nuanced and less severe in this subset of studies. For example, @obrien2012dyslexia used psychometric cutoffs across different tools and inclusion criteria, which may not necessarily have biased the results unless the selection criteria correlate with different underlying cognitive impairments (phonological vs. non-phonological). However, the authors themselves acknowledged this possibility, writing that *"depending on the dyslexia criterion used, samples may vary in composition with regard to their subtypes"* (p.33).

Overall, the presence of artificial admixture was most frequently associated with taxonic conclusions. Only two notable mismatches were identified: @marcus2012latent reached a dimensional conclusion regarding ADHD despite having incurred admixture of clinical cases, while @munson2008asd reached a taxonic conclusion about ASD subtypes without clear evidence of admixture (all children had received an ASD diagnosis based on what appeared to be a uniform clinical assessment). As discussed above, a third exception was observed in a secondary analysis of a subsample by @frazier2010asd targeting ASD. Taken together, these findings reinforce the view that ADHD is best conceptualized as a dimensional condition, while ASD may exhibit taxonic features, although the latter conclusion is supported by limited unbiased evidence.

In our simulated example, we found that artificial admixture represents a major threat to the validity of taxometric analysis, and to clustering methods as well. One might argue that our simulated scenario, in which half of the diagnosed cases were separately sampled, was extreme. However, this design is broadly consistent with the degree of admixture observed in the reviewed literature. It is worth noting that admixture is not limited to the obvious case of oversampling diagnosed individuals, and sometimes occurs in subtle ways. For example, through the undersampling of individuals with low trait levels, or by selectively drawing subsamples based on alternative clinical criteria within a larger dataset, as is often the case in subtype studies [cf. "subtractive" compound sampling, @haslam2020metaanalysis]. 

An important issue related to the present discussion concerns how we should treat traditional diagnostic categories such as those found in the DSM-5, which have recently come under criticism [@astle2022trandiagnostic; @sonuga2020kuhnian; @sonuga2022dontfit]. We argue that the question of whether clinically relevant conditions reflect discrete categories or multivariate, correlated continua does not necessarily undermine the practical utility of diagnostic systems. While the DSM-5 may seem to endorse a categorical model, it remains atheoretical and does not explicitly or scientifically commit to a taxonic view. Researchers, rather than diagnostic manuals, may risk reifying these categories as natural taxa, thus overinterpreting their conceptual status, hence we advocate for more high-quality taxometric studies. 

## Limitations and Future Directions

A first and obvious limitation of the present review concerns the small number of studies identified. This prevented us from conducting a meta-analysis: the available evidence was too scattered, methodologically heterogeneous, and inappropriate for quantitative synthesis. However, this limitation is itself informative, underscoring the need for more taxometric investigation in the field of NDDs. While the search query was comprehensive and no time limits were applied, more studies might have been included by relaxing the eligibility criteria. However, this is unlikely to have made a substantial difference. Exclusion criteria specified the lack of use of behavioral indicators, but no study was excluded on this basis. One record was screened out for being a dissertation [@clemons2006taxometric]. The full document could not be retrieved. Based on its abstract, it is clear that it focused on whether ADHD subtypes represent distinct taxa, and it reached a taxonic conclusion. However, some degree of admixture was possible (participants were separately recruited from clinical centers and schools).

A relevant future direction for this line of investigation concerns a systematic examination of the use and results of cluster analysis. As described above, some of the reviewed articles employed clustering methods, and clustering is a prominent tool in this field [@astle2022trandiagnostic]. We found that model-based approaches, such as latent class/profile analysis, were the most frequently used. The use of clustering is not without problems. First, while it is more flexible and can address broader questions, it does not offer a direct or consistent test of taxonicity [@beauchaine2003taxometrics; @ruscio2004boundaryissues]. Second, meaningful scientific inference from clustering (beyond its simple use as a data reduction technique) requires careful handling of assumptions, as the risk of detecting pseudoclusters is high [@toffalini2022entia; @toffalini2024clusters]. Third, it is is not immune to risks such as artificial admixture. Nonetheless, clustering and taxometric analysis could be more directly, but also critically, examined and compared when addressing the question of whether neurodevelopmental conditions reflect categories or dimensions.

# References

::: {#refs}
:::
